{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\limzh\\OneDrive\\Desktop\\NUS\\BT4222\\Project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Tensorflow Libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers #,models\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # Callback \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from sklearn.utils import compute_class_weight \n",
    "from keras.applications import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# System libraries\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from lime import submodular_pick\n",
    "\n",
    "# Oversampling SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Other Models\n",
    "from joblib import dump, load\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled\n",
    "train_labeled_dir = os.path.join(PATH, 'train')\n",
    "validation_labeled_dir = os.path.join(PATH, 'valid')\n",
    "test_labeled_dir = os.path.join(PATH, 'test')\n",
    "\n",
    "train_hostile_dir = os.path.join(train_labeled_dir, 'hostile_images')\n",
    "train_nonhostile_dir = os.path.join(train_labeled_dir, 'non_hostile_images')\n",
    "\n",
    "validation_hostile_dir = os.path.join(validation_labeled_dir, 'hostile_images')\n",
    "validation_nonhostile_dir = os.path.join(validation_labeled_dir, 'non_hostile_images')\n",
    "\n",
    "test_hostile_dir = os.path.join(test_labeled_dir, 'hostile_images')\n",
    "test_nonhostile_dir = os.path.join(test_labeled_dir, 'non_hostile_images')\n",
    "\n",
    "# Unlabeled\n",
    "train_dir = os.path.join(PATH, 'train_predict')\n",
    "valid_dir = os.path.join(PATH, 'valid_predict')\n",
    "test_dir = os.path.join(PATH, 'test_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training hostile images: 400\n",
      "total training non hostile images: 4501\n",
      "total validation hostile images: 100\n",
      "total validation non hostile images: 1126\n",
      "total test hostile images: 56\n",
      "total test non hostile images: 626\n",
      "--\n",
      "Total training images: 4901\n",
      "Total validation images: 1226\n",
      "Total test images: 682\n"
     ]
    }
   ],
   "source": [
    "num_hostile_tr = len(os.listdir(train_hostile_dir))\n",
    "num_nonhostile_tr = len(os.listdir(train_nonhostile_dir))\n",
    "num_hostile_val = len(os.listdir(validation_hostile_dir))\n",
    "num_nonhostile_val = len(os.listdir(validation_nonhostile_dir))\n",
    "num_nonhostile_test = len(os.listdir(test_nonhostile_dir))\n",
    "num_hostile_test = len(os.listdir(test_hostile_dir))\n",
    "total_train = num_hostile_tr + num_nonhostile_tr\n",
    "total_val = num_hostile_val + num_nonhostile_val\n",
    "total_test = num_hostile_test + num_nonhostile_test\n",
    "print('total training hostile images:', num_hostile_tr)\n",
    "print('total training non hostile images:', num_nonhostile_tr)\n",
    "print('total validation hostile images:', num_hostile_val)\n",
    "print('total validation non hostile images:', num_nonhostile_val)\n",
    "print('total test hostile images:', num_hostile_test)\n",
    "print('total test non hostile images:', num_nonhostile_test)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)\n",
    "print(\"Total test images:\", total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dir = {\n",
    "    train_dir: [train_hostile_dir, train_nonhostile_dir],\n",
    "    valid_dir: [validation_hostile_dir, validation_nonhostile_dir],\n",
    "    test_dir: [test_hostile_dir, test_nonhostile_dir]\n",
    "}\n",
    "\n",
    "for unlabeled, labeled_lst in unlabeled_dir.items():\n",
    "    if os.path.exists(unlabeled):\n",
    "        shutil.rmtree(unlabeled)\n",
    "    sub_dir = os.path.join(unlabeled, 'predict')\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "    for labeled in labeled_lst:\n",
    "        files = os.listdir(labeled)\n",
    "        dir_lst = labeled.split('\\\\')\n",
    "        for index, fileName in enumerate(files):\n",
    "            shutil.copy(os.path.join(labeled, fileName), os.path.join(sub_dir, f'{dir_lst[-1]}_{fileName}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255,rotation_range = 30, zoom_range = 0.20, \n",
    "                            fill_mode = \"nearest\", shear_range = 0.20, horizontal_flip = True, \n",
    "                            width_shift_range = 0.1, height_shift_range = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4901 images belonging to 2 classes.\n",
      "Found 1226 images belonging to 2 classes.\n",
      "Found 682 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generator for training and validation data\n",
    "train_labeled_image_generator = get_image_data_generator()\n",
    "validation_labeled_image_generator = get_image_data_generator()\n",
    "test_labeled_image_generator = get_image_data_generator()\n",
    "train_labeled_data_gen = train_labeled_image_generator.flow_from_directory(batch_size=batch_size, directory=train_labeled_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode='binary')\n",
    "val_labeled_data_gen = validation_labeled_image_generator.flow_from_directory(batch_size=batch_size,directory=validation_labeled_dir,target_size=(IMG_HEIGHT, IMG_WIDTH),class_mode='binary')\n",
    "test_labeled_data_gen = test_labeled_image_generator.flow_from_directory(batch_size=batch_size,directory=test_labeled_dir,target_size=(IMG_HEIGHT, IMG_WIDTH),class_mode='binary',shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4901 images belonging to 1 classes.\n",
      "Found 1226 images belonging to 1 classes.\n",
      "Found 682 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_generator = get_image_data_generator()\n",
    "validation_image_generator = get_image_data_generator()\n",
    "test_image_generator = get_image_data_generator()\n",
    "\n",
    "def get_image_generater(image_data_generator, directory):\n",
    "    return image_data_generator.flow_from_directory(batch_size=batch_size,directory=directory,target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=None,shuffle=False)\n",
    "\n",
    "train_data_gen = get_image_generater(train_image_generator, train_dir)\n",
    "val_data_gen = get_image_generater(validation_image_generator, valid_dir)\n",
    "test_data_gen = get_image_generater(test_image_generator, test_dir)\n",
    "train_data_gen.reset()\n",
    "val_data_gen.reset()\n",
    "test_data_gen.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6.12625, 1: 0.5444345700955343}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labeled_data_gen.classes),\n",
    "                                        y = train_labeled_data_gen.classes                                                    \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(train_labeled_data_gen.classes), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"classification_model_checkpoint\"\n",
    "checkpoint_callback = ModelCheckpoint(checkpoint_path,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor=\"val_accuracy\",\n",
    "                                      save_best_only=True)\n",
    "\n",
    "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
    "early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kerastuner as kt\n",
    "\n",
    "# def build_model(hp):\n",
    "#     # Initialize sequential API and start building model.\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)))\n",
    "    \n",
    "#     # Tune the number of hidden layers and units in each.\n",
    "#     # Number of hidden layers: 1 - 5\n",
    "#     # Number of Units: 32 - 512 with stepsize of 32\n",
    "#     for i in range(1, hp.Int(\"num_layers\", 2, 5)):\n",
    "#         model.add(\n",
    "#             keras.layers.Dense(\n",
    "#                 units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "#                 activation=\"relu\")\n",
    "#             )\n",
    "        \n",
    "#         # Tune dropout layer with values from 0 - 0.3 with stepsize of 0.1.\n",
    "#         model.add(keras.layers.Dropout(hp.Float(\"dropout_\" + str(i), 0, 0.3, step=0.1)))\n",
    "    \n",
    "#     # Add output layer.\n",
    "#     model.add(keras.layers.Dense(units=2, activation=\"softmax\"))\n",
    "    \n",
    "#     # Tune learning rate for Adam optimizer with values from 0.01, 0.001, or 0.0001\n",
    "#     hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "#     # Define optimizer, loss, and metrics\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "#                   loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "#                   metrics=[\"accuracy\"])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# tuner = kt.Hyperband(build_model,\n",
    "#                      objective=\"val_accuracy\",\n",
    "#                      max_epochs=20,\n",
    "#                      factor=3,\n",
    "#                      hyperband_iterations=10,\n",
    "#                      directory=\"kt_dir\",\n",
    "#                      project_name=\"kt_hyperband\",)\n",
    "\n",
    "# tuner.search_space_summary()\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# tuner.search(train_labeled_data_gen, epochs=epochs, callbacks=[stop_early], verbose=2, validation_data=val_labeled_data_gen)\n",
    "# best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# # Build model\n",
    "# model_custom = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# # Train the hypertuned model\n",
    "# model_custom.fit(train_labeled_data_gen, epochs=epochs, callbacks=[stop_early], verbose=2, validation_data=val_labeled_data_gen)\n",
    "# model_custom.save('model_custom.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)))\n",
    "    model.add(MaxPooling2D()) \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu')) \n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='relu')) \n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Flattening our dimensions\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 75, 75, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 75, 75, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 37, 37, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 37, 37, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 37, 37, 128)       73856     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 175232)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               22429824  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,523,201\n",
      "Trainable params: 22,523,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "39/39 [==============================] - 193s 5s/step - loss: 1.3836 - accuracy: 0.7029 - val_loss: 0.6367 - val_accuracy: 0.9299\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - 181s 5s/step - loss: 0.6751 - accuracy: 0.9155 - val_loss: 0.6527 - val_accuracy: 0.7667\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - 175s 4s/step - loss: 0.6327 - accuracy: 0.8313 - val_loss: 0.5226 - val_accuracy: 0.9462\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - 171s 4s/step - loss: 0.6061 - accuracy: 0.8464 - val_loss: 0.5060 - val_accuracy: 0.9168\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - 173s 4s/step - loss: 0.5231 - accuracy: 0.8372 - val_loss: 0.5228 - val_accuracy: 0.7635\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - 172s 4s/step - loss: 0.4602 - accuracy: 0.8341 - val_loss: 0.3874 - val_accuracy: 0.8385\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - 188s 5s/step - loss: 0.4409 - accuracy: 0.8386 - val_loss: 0.3860 - val_accuracy: 0.8711\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - 179s 5s/step - loss: 0.3956 - accuracy: 0.8653 - val_loss: 0.2907 - val_accuracy: 0.9184\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - 176s 4s/step - loss: 0.3464 - accuracy: 0.8727 - val_loss: 0.2711 - val_accuracy: 0.9046\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - 178s 5s/step - loss: 0.4271 - accuracy: 0.8257 - val_loss: 0.3595 - val_accuracy: 0.9339\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - 170s 4s/step - loss: 0.3448 - accuracy: 0.8837 - val_loss: 0.4741 - val_accuracy: 0.8320\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - 186s 5s/step - loss: 0.2917 - accuracy: 0.8937 - val_loss: 0.3138 - val_accuracy: 0.8964\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - 171s 4s/step - loss: 0.2798 - accuracy: 0.8772 - val_loss: 0.5230 - val_accuracy: 0.7798\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - 175s 4s/step - loss: 0.2782 - accuracy: 0.8900 - val_loss: 0.4327 - val_accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "model3 = get_model3()\n",
    "history3 = model3.fit(\n",
    "    train_labeled_data_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_labeled_data_gen,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Loss: 0.28830\n",
      "Test Accuracy: 89.15%\n"
     ]
    }
   ],
   "source": [
    "results = model3.evaluate(test_labeled_data_gen, verbose=0)\n",
    "\n",
    "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model_custom.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_labels(filename):\n",
    "    actual_labels = filename.split('\\\\')[-1].split('_')[0]\n",
    "    return 0 if actual_labels == 'hostile' else 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               819300    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100)              400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 819,902\n",
      "Trainable params: 819,702\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "154/154 [==============================] - 4s 19ms/step - loss: 0.3797 - accuracy: 0.8351 - val_loss: 0.1054 - val_accuracy: 0.9470\n",
      "Epoch 2/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.0936 - accuracy: 0.9700 - val_loss: 0.0495 - val_accuracy: 0.9829\n",
      "Epoch 3/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.0653 - accuracy: 0.9804 - val_loss: 0.0409 - val_accuracy: 0.9837\n",
      "Epoch 4/15\n",
      "154/154 [==============================] - 3s 18ms/step - loss: 0.0506 - accuracy: 0.9843 - val_loss: 0.0321 - val_accuracy: 0.9910\n",
      "Epoch 5/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.0231 - val_accuracy: 0.9935\n",
      "Epoch 6/15\n",
      "154/154 [==============================] - 3s 19ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.0191 - val_accuracy: 0.9943\n",
      "Epoch 7/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0340 - val_accuracy: 0.9878\n",
      "Epoch 8/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0300 - accuracy: 0.9916 - val_loss: 0.0215 - val_accuracy: 0.9943\n",
      "Epoch 9/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0331 - accuracy: 0.9900 - val_loss: 0.0225 - val_accuracy: 0.9943\n",
      "Epoch 10/15\n",
      "154/154 [==============================] - 3s 18ms/step - loss: 0.0337 - accuracy: 0.9888 - val_loss: 0.0181 - val_accuracy: 0.9935\n",
      "Epoch 11/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0305 - accuracy: 0.9910 - val_loss: 0.0232 - val_accuracy: 0.9935\n",
      "Epoch 12/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0306 - accuracy: 0.9892 - val_loss: 0.0248 - val_accuracy: 0.9918\n",
      "Epoch 13/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0388 - accuracy: 0.9871 - val_loss: 0.0223 - val_accuracy: 0.9935\n",
      "Epoch 14/15\n",
      "154/154 [==============================] - 3s 18ms/step - loss: 0.0337 - accuracy: 0.9882 - val_loss: 0.0157 - val_accuracy: 0.9951\n",
      "Epoch 15/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.0344 - accuracy: 0.9884 - val_loss: 0.0203 - val_accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "def get_model2():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(4,4,512)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "pretrained_model = VGG16(include_top=False, weights='imagenet')\n",
    "pretrained_model.summary()\n",
    "vgg_features_train = pretrained_model.predict_generator(train_data_gen)\n",
    "vgg_features_val = pretrained_model.predict_generator(val_data_gen)\n",
    "vgg_features_test = pretrained_model.predict_generator(test_data_gen)\n",
    "train_target = to_categorical(list(map(get_actual_labels, train_data_gen.filenames)))\n",
    "val_target = to_categorical(list(map(get_actual_labels, val_data_gen.filenames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               819300    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 819,902\n",
      "Trainable params: 819,702\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "154/154 [==============================] - 5s 20ms/step - loss: 0.3834 - accuracy: 0.7776 - val_loss: 0.0956 - val_accuracy: 0.9821\n",
      "Epoch 2/15\n",
      "154/154 [==============================] - 3s 21ms/step - loss: 0.1830 - accuracy: 0.9380 - val_loss: 0.1510 - val_accuracy: 0.9527\n",
      "Epoch 3/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.1390 - accuracy: 0.9498 - val_loss: 0.0391 - val_accuracy: 0.9894\n",
      "Epoch 4/15\n",
      "154/154 [==============================] - 3s 18ms/step - loss: 0.0898 - accuracy: 0.9635 - val_loss: 0.0257 - val_accuracy: 0.9935\n",
      "Epoch 5/15\n",
      "154/154 [==============================] - 3s 19ms/step - loss: 0.0733 - accuracy: 0.9749 - val_loss: 0.0731 - val_accuracy: 0.9715\n",
      "Epoch 6/15\n",
      "154/154 [==============================] - 3s 19ms/step - loss: 0.0563 - accuracy: 0.9804 - val_loss: 0.0529 - val_accuracy: 0.9804\n",
      "Epoch 7/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0563 - accuracy: 0.9814 - val_loss: 0.0685 - val_accuracy: 0.9747\n",
      "Epoch 8/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0507 - accuracy: 0.9804 - val_loss: 0.0188 - val_accuracy: 0.9935\n",
      "Epoch 9/15\n",
      "154/154 [==============================] - 3s 19ms/step - loss: 0.0454 - accuracy: 0.9831 - val_loss: 0.0269 - val_accuracy: 0.9927\n",
      "Epoch 10/15\n",
      "154/154 [==============================] - 3s 20ms/step - loss: 0.0424 - accuracy: 0.9822 - val_loss: 0.0260 - val_accuracy: 0.9910\n",
      "Epoch 11/15\n",
      "154/154 [==============================] - 3s 18ms/step - loss: 0.0423 - accuracy: 0.9818 - val_loss: 0.0502 - val_accuracy: 0.9796\n",
      "Epoch 12/15\n",
      "154/154 [==============================] - 3s 17ms/step - loss: 0.0342 - accuracy: 0.9847 - val_loss: 0.0630 - val_accuracy: 0.9763\n",
      "Epoch 13/15\n",
      "154/154 [==============================] - 3s 19ms/step - loss: 0.0418 - accuracy: 0.9818 - val_loss: 0.0429 - val_accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "model2 = get_model2()\n",
    "history2 = model2.fit(\n",
    "    vgg_features_train,\n",
    "    train_target,\n",
    "    epochs=epochs,\n",
    "    validation_data=(vgg_features_val, val_target),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Loss: 0.02139\n",
      "Test Accuracy: 99.56%\n"
     ]
    }
   ],
   "source": [
    "results = model2.evaluate(x=vgg_features_test,y=to_categorical(list(map(get_actual_labels, test_data_gen.filenames))), verbose=0)\n",
    "\n",
    "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "total_images = train_labeled_data_gen.n  \n",
    "steps = total_images//batch_size \n",
    "\n",
    "x_train , y_train = [] , []\n",
    "for i in range(steps):\n",
    "    a , b = train_labeled_data_gen.next()\n",
    "    x_train.extend(a) \n",
    "    y_train.extend(b)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "nsamples, nx, ny, nrgb = x_train.shape\n",
    "x_train2 = x_train.reshape((nsamples,nx*ny*nrgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc=DecisionTreeClassifier()\n",
    "dtc.fit(x_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DT.joblib']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(dtc, './other_models/DT.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RandomForestClassifier()\n",
    "model.fit(x_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF.joblib']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, './other_models/RF.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(x_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KNN.joblib']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(knn, './other_models/KNN.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "nb.fit(x_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NB.joblib']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(nb, './other_models/NB.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipping the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./other_models\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "directory_to_zip = './other_models'\n",
    "zip_file_path = 'other_models.zip'\n",
    "with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "    print(directory_to_zip)\n",
    "    for root, directories, files in os.walk(directory_to_zip):\n",
    "        for file in files:\n",
    "            print(1)\n",
    "            zip_file.write(os.path.join(root, file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
